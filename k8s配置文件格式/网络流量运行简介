1.外网接入流程：https://blog.csdn.net/yang75108/article/details/101268208
2.服务发现和负载均衡：https://blog.csdn.net/yang75108/article/details/101267444
3.pod虚拟机互通：https://blog.csdn.net/yang75108/article/details/101101384

注：每一层都是构建于前一层之上

第三层：外部网络接入（外部流量接入）
【NodePort、LoadBlancer、Ingress】
第二层：service网络（服务发现和负载均衡）
【Cluster IP + Port】
第一层：pod网络（pod虚拟机互通互联）
【Pord IP + Port】
第0层：Node节点网络（节点主机互通，一般不用管，都是通的）
【Node IP + Port】

一、第0层
第0层Node节点网络比较好理解，也就是保证K8s节点(物理或虚拟机)之间能够正常IP寻址和互通的网络

二、第一层（https://blog.csdn.net/yang75108/article/details/101101384）
1.Pod相当于是K8s云平台中的虚拟机，它是K8s的基本调度单位。Pod网络构建于Node节点网络之上，它又是上层Service网络的基础。
2.所谓Pod网络，就是能够保证K8s集群中的所有Pods(包括同一节点上的，也包括不同节点上的Pods)，能够相互做IP寻址和通信的网络。

3.流量：
host：eth0（nodeIP）——>docker0（nodeDockerIP）——>veth0（单个pod内的虚拟ip用于连接pod内的container和pause）

eth0是节点主机上的网卡，这个是支持该节点流量出入的设备，也是支持集群节点间IP寻址和互通的设备
docker0是一个虚拟网桥，可以简单理解为一个虚拟交换机，它是支持该节点上的Pod之间进行IP寻址和互通的设备
veth0则是单个Pod内的虚拟网卡，是支持该Pod内容器互通和对外访问的虚拟设备
（Pod1中还有一个比较特殊的叫pause的容器，这个容器运行的唯一目的是为Pod建立共享的veth0网络接口）

4.网络插件
flannel/weavenet：采用隧道封包技术
CNI(Container Network Interface)


总结：K8s的网络可以抽象成四层网络，第0层节点网络，第1层Pod网络，第2层Service网络，第3层外部接入网络。除了第0层，每一层都构建于上一层之上。
一个节点内的Pod网络依赖于虚拟网桥和虚拟网卡等linux虚拟设备，保证同一节点上的Pod之间可以正常IP寻址和互通。一个Pod内容器共享该Pod的网络栈，这个网络栈由pause容器创建。
不同节点间的Pod网络，可以采用路由方案实现，也可以采用覆盖网络方案。路由方案依赖于底层网络设备，但没有额外性能开销，覆盖网络方案不依赖于底层网络，但有额外封包解包开销。
CNI是一个Pod网络集成标准，简化K8s和不同Pod网络实现技术的集成。

三、第二层（https://blog.csdn.net/yang75108/article/details/101267444）

一个Service背后一般由多个Pods组成集群，这时候就引入了服务发现(Service Discovery)和负载均衡(Load Balancing)等问题

1.流量一
k8s Cluster：pod（client）——>AccountService(提供访问pod集群的虚拟ip：clusterIP)——>podIPs（pod集群，pod1、pod2、pod3...）

服务发现：Account-Service提供统一的ClusterIP来解决服务发现问题，Client只需通过ClusterIP就可以访问Account-App的Pod集群，不需要关心集群中的具体Pod数量和PodIP，
即使是PodIP发生变化也会被ClusterIP所屏蔽。
注意，这里的ClusterIP实际是个虚拟IP，也称Virtual IP(VIP)

负载均衡：Account-Service抽象层具有负载均衡的能力，支持以不同策略去访问pod集群中的不同Pod实例，以实现负载分摊和HA高可用。
K8s中默认的负载均衡策略是RoundRobin，也可以定制其它复杂策略

2.流量二
dns域名解析：引入Service Registry+Client配合实现，在当下微服务时代，这是一个比较流行的做法。
目前主流的产品，如Netflix开源的Eureka + Ribbon，HashiCorp开源的Consul，还有阿里新开源Nacos等，都是这个方案的典型代表

k8s Cluster：pod集群（ip + port）——>自动注册到Service Registry
             client（查找目标pod）——>Service Registry（充当注册中心）
在K8s中引入Service Registry实现服务发现也不复杂，K8s自身带分布式存储etcd就可以实现Service Registry。
假设K8s引入Service Registry做服务发现(如上所示)，运行时K8s可以把服务和Pod集群信息(IP + Port等)自动注册到Service Registry，
Client应用则通过Service Registry查询发现目标Pod，然后发起调用。
这个方案也不复杂，而且客户端可以实现灵活的负载均衡策略，但是需要引入客户端配合，对客户应用有侵入性，所以K8s也没有直接采用这种方案

3.流量三（服务发现）
在K8s平台的每个Worker节点上，都部署有两个组件，一个叫Kubelet，另外一个叫Kube-Proxy，这两个组件+Master是K8s实现服务注册和发现的关键。

重要：（服务发现原理）
首先，在服务Pod实例发布时(可以对应K8s发布中的Kind: Deployment)，Kubelet会负责启动Pod实例，启动完成后，Kubelet会把服务的PodIP列表汇报注册到Master节点。
其次，通过服务Service的发布(对应K8s发布中的Kind: Service)，K8s会为服务分配ClusterIP，相关信息也记录在Master上。
第三，服务发现时，Kube-Proxy监听Master并发现服务ClusterIP和PodIP列表映射关系，修改本地的linux iptables转发规则，指示iptables在接收到某个ClusterIP请求时，负载均衡的转发到对应的PodIP上
运行时，当有消费者Pod需要访问某个目标服务实例的时候，它通过ClusterIP发起调用，这个ClusterIP会被本地iptables机制截获，然后通过负载均衡，转发到目标服务Pod实例上。

注册：kubelet（deployment——podIP）——>注册到master节点上
      service（service——ClusterIP）——>同样注册到master节点上（与podIP形成映射关系）
      
发现：kube-proxy——>监听master——>发现podIP和ClusterIP的映射关系——通过IPtable规则接收其他ClusterIP请求——负载均衡到对应的podIP上

注：实际消费者Pod也并不直接调服务的ClusterIP，而是先调用服务名，因为ClusterIP也会变，只有服务名一般不变。
为了屏蔽ClusterIP的变化，K8s在每个Worker节点上还引入了一个KubeDNS组件，它也监听Master并发现服务名和ClusterIP之间映射关系，这样， 消费者Pod通过KubeDNS可以间接发现服务的ClusterIP

总结：
1、K8s的Service网络构建于Pod网络之上，它主要目的是解决服务发现(Service Discovery)和负载均衡(Load Balancing)问题。
2、K8s通过一个ServiceName+ClusterIP统一屏蔽服务发现和负载均衡，底层技术是在DNS+Service Registry基础上发展演进出来。
3、K8s的服务发现和负载均衡是在客户端通过Kube-Proxy + iptables转发实现，它对应用无侵入，且不穿透Proxy，没有额外性能损耗。
4、K8s服务发现机制，可以认为是现代微服务发现机制和传统Linux内核机制的优雅结合

三、第三层（https://blog.csdn.net/yang75108/article/details/101268208）

1.服务暴露（NodePort）
Kube-Proxy在K8s集群中所有Worker节点上都部署有一个，它掌握Service网络的所有信息，知道怎么和Service网络以及Pod网络互通互联。
如果要将Kube-Proxy和节点网络打通(从而将某个服务通过Kube-Proxy暴露出去)，只需要让Kube-Proxy在节点上暴露一个监听端口即可。
这种通过Kube-Proxy在节点上暴露一个监听端口，将K8s内部服务通过Kube-Proxy暴露出去的方式，术语就叫NodePort(顾名思义，端口暴露在节点上)
要将K8s内部的一个服务通过NodePort方式暴露出去，可以将服务发布(kind: Service)的type设定为NodePort，同时指定一个30000~32767范围内的端口
服务发布以后，K8s在每个Worker节点上都会开启这个监听端口。这个端口的背后是Kube-Proxy，当K8s外部有Client要访问K8s集群内的某个服务，
它通过这个服务的NodePort端口发起调用，这个调用通过Kube-Proxy转发到内部的Servcie抽象层，然后再转发到目标Pod上。

client（访问服务的NodePort端口）——kube-proxy（转发到内部的service）——ClusterIP——podIP

2.服务暴露（LoadBalancer）一般不用
3.服务暴露（ingress）

Ingress就是一个特殊的Service，通过节点的**HostPort(80/443)**暴露出去，前置一般也有LB做负载均衡。
Ingress转发到内部的其它服务，是通过集群内的Service抽象层/ClusterIP进行转发，最终转发到目标服务Pod上

首先，它本质上就是K8s集群中的一个比较特殊的Service(发布Kind: Ingress)。
其次，这个Service提供的功能主要就是7层反向代理(也可以提供安全认证，监控，限流和SSL证书等高级功能)，功能类似Nginx。
第三，这个Service对外暴露出去是通过HostPort(80/443)，可以和上面LoadBalancer对接起来。

总结：
1、odePort是K8s内部服务对外暴露的基础，LoadBalancer底层有赖于NodePort。NodePort背后是Kube-Proxy，Kube-Proxy是沟通Service网络、Pod网络和节点网络的桥梁。
2、K8s服务通过NodePort对外暴露是以集群方式暴露的，每个节点上都会暴露相应的NodePort，通过LoadBalancer可以实现负载均衡访问。
公有云(如阿里云/AWS/GCP)提供的K8s，都支持自动部署LB，且提供公网可访问IP，LB背后对接NodePort。
3、ngress扮演的角色是对K8s内部服务进行集中反向代理，通过Ingress，我们可以同时对外暴露K8s内部的多个服务，但是只需要购买1个(或者少量)LB。
Ingress本质也是一种K8s的特殊Service，它也通过HostPort(80/443)对外暴露。
4、通过Kubectl Proxy或者Port Forward，可以在本地环境快速调试访问K8s中的服务或Pod。
5、k8s的Service发布主要有3种type，type=ClusterIP，表示仅内部可访问服务，type=NodePort，表示通过NodePort对外暴露服务，type=LoadBalancer，表示通过LoadBalancer对外暴露服务(底层对接NodePort，一般公有云才支持)。

